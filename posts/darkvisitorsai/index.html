<!DOCTYPE html>
<html lang="en">

    <head><title>Using Dark Visitors to Safeguard from AI Bot Agents &ndash; tox@box</title>
<meta name="description" content="A driven 18 y/o [network engineering](https://www.linkedin.com/in/damon-hoody/) student from ohio looking for new opportunities to learn and grow :&gt;">

<meta name="viewport" content="width=device-width, initial-scale=1">
<meta charset="UTF-8"/>



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css" integrity="sha512-1sCRPdkRXhBV2PBLUdRb4tMg1w2YPf37qatUFeS7zlBy7jJI8Lf4VHwWfZZfpXtYSLy85pkm9GaYVYMfw5BC1A==" crossorigin="anonymous" />


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin="anonymous" />


<link rel="stylesheet" href="https://hoody.cx/css/palettes/windows-custom.css">
<link rel="stylesheet" href="https://hoody.cx/css/risotto.css">
<link rel="stylesheet" href="https://hoody.cx/css/custom.css">


<link rel="icon" href="https://hoody.cx/favicon.ico">




</head>

    <body>
        <div class="page">

            <header class="page__header"><nav class="page__nav main-nav">
    <ul>
    <h1 class="page__logo"><a href="https://hoody.cx/" class="page__logo-inner">tox@box</a></h1>
    
    
    <li class="main-nav__item"><a class="nav-main-item" href="https://hoody.cx/about/" title="">About</a></li>
    
    <li class="main-nav__item"><a class="nav-main-item" href="https://hoody.cx/contact/" title="">Contact</a></li>
    
    <li class="main-nav__item"><a class="nav-main-item" href="https://drive.google.com/file/d/1Jzzn_-tv3IeXcNIx5OwOwStMi1rYXY5m/view?usp=sharing" title="">Resume</a></li>
    
    <li class="main-nav__item"><a class="nav-main-item active" href="https://hoody.cx/posts/" title="Posts">Posts</a></li>
    
    </ul>
</nav>

</header>

            <section class="page__body">
    <header class="content__header">
        <h1>Using Dark Visitors to Safeguard from AI Bot Agents</h1>
    </header>
    
    <div class="content__body">
        <hr>
<p><img src="/robots.JPG" alt="screenshot"></p>
<p>In the ever-evolving landscape of the internet, websites are constantly under the watchful eyes of various entities, including search engines and automated bots. While search engines play a crucial role in driving organic traffic to your site, not all bots have good intentions. Some automated agents, powered by artificial intelligence (AI), may scrape your website content for malicious purposes, including content theft, data mining, SEO manipulation, price scraping, vulnerability scanning, DDoS attacks, etc. This is where the humble <code>robots.txt</code> file steps in as your first line of defense.</p>
<h1 id="what-is-robotstxt">What is <code>robots.txt</code>?</h1>
<p>The robots.txt file is a web standard used by sites to communicate with web crawlers and other automated agents, specifying which parts of the site should not be crawled or analyzed. It serves as a set of instructions for web robots &amp; AI, outlining the areas they are allowed to access and those they should avoid. Essentially, it&rsquo;s a virtual &ldquo;DO NOT ENTER&rdquo; sign for bots.</p>
<h1 id="crafting-an-effective-robotstxt-file-with-dark-visitors">Crafting an Effective <code>robots.txt</code> File with Dark Visitors</h1>
<p>Dark Visitors is a curated list of known AI agents currently roaming the internet (see script below to parse the list). It may be accessed at <a href="https://darkvisitors.com">darkvisitors.com</a>. In order to utilize this list, you must create a <code>robots.txt</code> text file within the root of your website (investigate how to do this for your website solution). For hugo users, add <code>enableRobotsTXT = true</code> to your <code>config.toml</code> and make a robots.txt file in /layouts.</p>
<p>In order to customize this list into something like the screenshot at the top of this site, we must decide which agents to block, and which to allow through. I have crafted a quick payload on my linux machine with the command-line tool cURL to do this for me:</p>
<pre tabindex="0"><code>curl -s https://darkvisitors.com/agents | grep -o &#39;&lt;div class=&#34;name agent-name&#34;&gt;.*&lt;/div&gt;&#39; | sed &#39;s/&lt;div class=&#34;name agent-name&#34;&gt;\(.*\)&lt;\/div&gt;/\1/&#39;
</code></pre><p>Essentially, this script parses the site for an updated list of all of the agents that are currently tracked by the website &amp; sorts the content with <code>grep</code> and <code>sed</code>.</p>
<p>Personally, I do not block all of these agents, just the most important AI data scrapers for simpler bot protection. Here is the robots list that I currently use (also in this sites <a href="https://pwnedbytoxic.github.io/robots.txt">/robots.txt</a> :D)</p>
<pre tabindex="0"><code>User-agent: anthropic-ai
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: FacebookBot
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: GPTBot
Disallow: /

User-agent: omgilibot
Disallow: /

User-agent: omgili
Disallow: /
</code></pre><p>Again, you can customize this list any way you want, but this configuration makes me feel a bit more secure in this crazy age of mass-botting &amp; web-scraping AI.</p>
<h1 id="testing-full-functionality">Testing Full Functionality</h1>
<p>As a quick way to demonstrate the power of this agent whitelisting, I am going to pass this post&rsquo;s URL into ChatGPT 4 &amp; see how it reacts.</p>
<p><img src="/robots2.JPG" alt="GPT4"></p>
<p>Perfect.</p>
<hr>
<p>Happy Hacking!</p>

    </div>
    <footer class="content__footer"></footer>

            </section>

            <section class="page__aside">
                <div class="aside__about">
<div class="aside__about">
    <img class="about__logo" src="https://hoody.cx/images/rice.svg" alt="Logo">
<h1 class="about__title">pwnedbytoxic</h1>
<p class="about__description">A driven 18 y/o <a href="https://www.linkedin.com/in/damon-hoody/">network engineering</a> student from ohio looking for new opportunities to learn and grow :&gt;</p>
</div>


<ul class="aside__social-links">
    
    <li>
        <a href="https://www.linkedin.com/in/damon-hoody/" rel="me" aria-label="LinkedIn" title="LinkedIn"><i class="fa-brands fa-linkedin-in" aria-hidden="true"></i></a>&nbsp;
    </li>
    
    <li>
        <a href="https://twitter.com/t0x_tech" rel="me" aria-label="Twitter" title="Twitter"><i class="fa-brands fa-twitter" aria-hidden="true"></i></a>&nbsp;
    </li>
    
    <li>
        <a href="https://infosec.exchange/@toxbox" rel="me" aria-label="Mastodon" title="Mastodon"><i class="fa-brands fa-mastodon" aria-hidden="true"></i></a>&nbsp;
    </li>
    
    <li>
        <a href="https://github.com/pwnedbytoxic" rel="me" aria-label="GitHub" title="GitHub"><i class="fa-brands fa-github" aria-hidden="true"></i></a>&nbsp;
    </li>
    
    <li>
        <a href="https://tryhackme.com/p/toxic.sec" rel="me" aria-label="TryHackMe" title="TryHackMe"><i class="fa fa-flag" aria-hidden="true"></i></a>&nbsp;
    </li>
    
</ul>
</div>
                <hr>
                <div class="aside__content">
    
    
        <p>
            
            2024-01-06
        </p>
    

    

                </div>
            </section>

            <footer class="page__footer"><p>
    
    
    
    
    
    
      
    
      
    
      
    
      
    
    
    
      
      
          
            
            
                <br/><span class="active">$ echo $LANG<br/><b></b></span><br/>

            
          
      
    
</p>
<br /><br />
<p class="copyright"></p>
<p class="advertisement">// made by toxic //</p>
</footer>

        </div>
    </body>

</html>
